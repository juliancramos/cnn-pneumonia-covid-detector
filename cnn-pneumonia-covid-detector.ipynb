{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliancramos/cnn-pneumonia-covid-detector/blob/main/cnn-pneumonia-covid-detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6qgTv5glRVh"
      },
      "source": [
        "# Chest X-Ray Classification: Normal vs. Pneumonia vs. COVID-19\n",
        "\n",
        "**Authors:** Sergio Ortíz - Julián Ramos - Melissa Ruíz\n",
        "\n",
        "**Course:** Machine Learning Techniques\n",
        "\n",
        "**Date:** February 2026\n",
        "\n",
        "## 1. Executive Summary\n",
        "Lung diseases such as Pneumonia and COVID-19 are critical health conditions requiring timely diagnosis. Traditional manual interpretation of chest X-rays is subjective and prone to error.\n",
        "\n",
        "This project aims to develop an automated Deep Learning system using **Convolutional Neural Networks (CNNs)** to classify chest X-ray images into three categories:\n",
        "1.  **Normal**\n",
        "2.  **Pneumonia**\n",
        "3.  **COVID-19**\n",
        "\n",
        "---\n",
        "### 1.1 Project Objectives\n",
        "This project follows a structured workflow designed to meet specific technical requirements:\n",
        "\n",
        "1.  **Exploratory Data Analysis (EDA):**\n",
        "    * Explore the dataset to identify key characteristics, including class distribution and image quality.\n",
        "    * Detect potential biases (e.g., class imbalance) and present visualizations to support design decisions.\n",
        "\n",
        "2.  **Image Preprocessing:**\n",
        "    * Implement techniques to improve input quality, such as pixel normalization and resizing.\n",
        "    * Apply **Data Augmentation** strategies to improve model generalization and prevent overfitting.\n",
        "    * Justify how these steps prepare the data for the neural network.\n",
        "\n",
        "3.  **Neural Network Design:**\n",
        "    * Design a custom **Convolutional Neural Network (CNN)** architecture suitable for classification.\n",
        "    * Justify the selection of layers (Convolutional, Pooling, Fully Connected), filter sizes, and activation functions.\n",
        "    * Present the architecture diagram and explain its expected behavior.\n",
        "\n",
        "4.  **Training and Evaluation:**\n",
        "    * Train the model using an appropriate optimization strategy and loss function.\n",
        "    * Evaluate performance on a validation set using robust metrics such as **Precision, Recall, and F1-Score**.\n",
        "    * Analyze results to identify issues like overfitting and propose technical solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRyeKxrx3sdL"
      },
      "source": [
        "## 2. Environment Setup and Configuration\n",
        "\n",
        "In this section, the execution environment is configured.\n",
        "\n",
        "### 2.1 Technical Stack\n",
        "* **Data Manipulation:** `pandas` and `numpy` are utilized for metadata handling and numerical operations.\n",
        "* **Visualization:** `matplotlib` and `seaborn` are employed to analyze data distributions and training metrics.\n",
        "* **Image Processing:** `cv2` (OpenCV) is used for reading and resizing X-ray images.\n",
        "* **Deep Learning:** `tensorflow` and `keras` serve as the core framework for constructing the CNN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI4Zhvu63Hlz",
        "outputId": "256850bf-2fc6-46aa-c623-27be634cc021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment setup complete.\n",
            "TensorFlow Version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "# CONSTANTS CONFIGURATION\n",
        "# Fixed seed to ensure reproducibility (weights)\n",
        "SEED = 42\n",
        "# Target image resolution (224x224 pixels)\n",
        "IMG_SIZE = 224\n",
        "# Number of images processed per iteration\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def set_seed(seed_value):\n",
        "    \"\"\"\n",
        "    Sets the random seed for reproducibility across all libraries.\n",
        "\n",
        "    This function locks the pseudo-random number generators of Python, NumPy,\n",
        "    and TensorFlow to a specific sequence.\n",
        "\n",
        "    Args:\n",
        "        seed_value (int): The seed number to initialize generators.\n",
        "    \"\"\"\n",
        "    # Python's built-in hashing (dictionaries/sets)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "\n",
        "    # Python's standard random library (shuffle, choice)\n",
        "    random.seed(seed_value)\n",
        "\n",
        "    # NumPy's random module (array splitting, matrices)\n",
        "    np.random.seed(seed_value)\n",
        "\n",
        "    # TensorFlow's random module (weight initialization)\n",
        "    tf.random.set_seed(seed_value)\n",
        "\n",
        "# Apply the configuration\n",
        "set_seed(SEED)\n",
        "\n",
        "# Visualization Setup\n",
        "# Set a style for all plots (background grid)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Define a global figure size (12x6 inches)\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Silence non-critical alerts (DeprecationWarnings)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Environment setup complete.\")\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Ingestion\n",
        "\n",
        "In this section, the required datasets are downloaded from Kaggle.\n",
        "\n",
        "The Kaggle API (`kaggle.json`) is utilized because the dataset consists of **unstructured image data** organized in directory trees, rather than structured tabular files (e.g., CSV or Excel).\n",
        "**Datasets acquired:**\n",
        "* **Chest X-Ray Images (Pneumonia):** provided by Paul Mooney.\n",
        "* **COVID-19 Radiography Database:** provided by Tawsifur Rahman."
      ],
      "metadata": {
        "id": "0mwA_EFpKGqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def setup_kaggle_from_drive():\n",
        "    \"\"\"\n",
        "    Configures the Kaggle API by loading the token from Google Drive.\n",
        "\n",
        "    This function mounts the user's Google Drive, looks for 'kaggle.json'\n",
        "    in the root directory ('/content/drive/MyDrive/'), and download the datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate if data already exists\n",
        "    if os.path.exists('chest_xray') and os.path.exists('COVID-19_Radiography_Dataset'):\n",
        "        print(\"Data already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Mount Google Drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Path to the token in Drive\n",
        "    drive_path = '/content/drive/MyDrive/kaggle.json'\n",
        "    #local_path = '/root/.kaggle/kaggle.json'\n",
        "\n",
        "    # Check and Copy Credentials\n",
        "    if not os.path.exists(drive_path):\n",
        "        print(f\"Error: 'kaggle.json' not found at {drive_path}.\")\n",
        "        print(\"Upload the file to your Google Drive root folder.\")\n",
        "        return\n",
        "\n",
        "    # Create the hidden directory .kaggle\n",
        "    !mkdir -p ~/.kaggle\n",
        "\n",
        "    # Copy the file from Drive to the local Colab environment\n",
        "    !cp \"$drive_path\" ~/.kaggle/\n",
        "\n",
        "    # Set security permissions (Required by Kaggle API)\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(\"API Token loaded from Drive successfully.\")\n",
        "\n",
        "    # 3. Dataset Download & Extraction\n",
        "    print(\"\\nDownloading Datasets...\")\n",
        "\n",
        "    print(\"Downloading Chest X-Ray Images (Pneumonia)...\")\n",
        "    !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia --force\n",
        "\n",
        "    print(\"Downloading COVID-19 Radiography Database...\")\n",
        "    !kaggle datasets download -d tawsifurrahman/covid19-radiography-database --force\n",
        "\n",
        "    print(\"\\nExtracting Files...\")\n",
        "    # Unzip\n",
        "    !unzip -qo chest-xray-pneumonia.zip\n",
        "    !unzip -qo covid19-radiography-database.zip\n",
        "\n",
        "    print(\"SUCCESS: Data ingestion complete.\")\n",
        "\n",
        "# Execute the function\n",
        "setup_kaggle_from_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJefCrQBKPOB",
        "outputId": "5e839367-cf41-46db-be8b-761cf3259e27"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already exists. Skipping download and extraction.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEfmU3qBhnGpt9olNpZSqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}